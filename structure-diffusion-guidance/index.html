
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>StructuredDiffusion</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->
    <meta property="og:image" content="https://weixi-feng.github.io/structurediffusion/img/method.jpg">
    <meta property="og:image:type" content="image/jpg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://weixi-feng.github.io/structurediffusion/"/>
    <meta property="og:title" content="structured-diffusion-guidance" />
    <meta property="og:description" content="Project page for Structured Diffusion Guidance for Compositional Text-to-Image Synthesis." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="structured-diffusion-guidance" />
    <meta name="twitter:description" content="Project page for Structured Diffusion Guidance for Compositional Text-to-Image Synthesis." />
    <meta name="twitter:image" content="https://weixi-feng.github.io/structurediffusion/img/.png" />


    <!-- <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Training-Free Structured Diffusion Guidance <br> for Compositional Text-to-Image Synthesis</br> </b>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://weixi-feng.github.io/">Weixi Feng</a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en">Xuehai He</a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://tsujuifu.github.io/">Tsu-Jui Fu</a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://varunjampani.github.io/">Varun Jampani</a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://www.arjunakula.com/">Arjun Akula</a><sup>3</sup>
                    </li><br>
                    <li>
                        <a href="https://scholar.google.com/citations?user=BV2dbjEAAAAJ&hl=en">Pradyumna Narayana</a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/sugatobasu/">Sugato Basu</a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a><sup>1</sup>
                    </li>
                </ul>
                <sup>1</sup>UC Santa Barbara, <sup>2</sup>UC Santa Cruz, <sup>3</sup>Google
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/">
                            <image src="img/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/weixi-feng/Structured-Diffusion-Guidance-for-Compositional-T2I">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/huggingface.png" height="60px">
                                <h4><strong>Demo [Coming Soon]</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#results">
                            <image src="img/result_pointer.png" height="60px"></image>
                                <h4><strong>Results</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    We improve the compositional skills of T2I models, specifically more accurate attribute binding and better image compositions. 
                    To achieve this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. 
                    We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. 
                    Therefore, we can better preserve the compositional semantics in the generated image by manipulating the cross-attention representations based on linguistic insights. 
                    Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. 
                    We achieve better compositional skills in qualitative and quantitative results, leading to a 5-8% advantage in head-to-head user comparison studies. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="results">
                    Results
                </h3>
                <hr />
                <h4>Concept Conjunction: two objects with different colors.</h4>
                <image src="img/result1.png" class="img-responsive" alt="overview"></image>
                <hr />
                <h4>General prompts with multiple objects or colors</h4>
                <image src="img/result2.jpg" class="img-responsive" alt="overview"></image>
            </div>
        </div>


        <hr />

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method 
                </h3>
                <h4>
                    Cross Attention Control
                </h4>
                <image src="img/attention.jpg" class="img-responsive" alt="overview"></image>
                <p class="text-justify">
                    The spatial layouts depend on the cross attention maps. These maps control the layout and structure of generated images, while the values contain rich semantics mapped into attended regions. 
                    We assume that the image layout and content can be disentangled by controlling attention maps and values separately. (See <a href="#related">prompt-to-prompt</a>)
                </p>
                <h4>
                    Structured Diffusion Guidance
                </h4>
                <image src="img/method.jpg" class="img-responsive" alt="overview">
                <p class="text-justify">
                    We fuse the <strong>structured representations</strong> (e.g. constituency tree or scene graph) into the guidance process by encoding the individual concepts (i.e. noun phrases) separately.
                    Features from these individual concepts are used to replace features from the full input prompt. The semantics of each individual words is enhanced after such replacement.
                    In each cross-attention layer, the keys are computed from the unmodified prompt features while the values are from multiple replaced features. 
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Analysis
                </h3>
                <p>More to come. Please refer to the paper (appendix) for now.</p>
                <image src="img/a_brown_bird_and_a_blue_bear.png" alt="overview" height="200px"></image>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p>                
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/ship_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/chair_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/mic_sbs_path1.mp4" type="video/mp4" />
                </video>
                <br><br>
                <p class="text-justify">
                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:
                </p>     
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />
                </video>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="related">
                    Related Works
                </h3>
                <p class="text-justify">
                    <a href="https://github.com/CompVis/stable-diffusion/tree/69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc">Stable Diffusion</a> and <a href="https://arxiv.org/abs/2112.10752">Latent Diffusion Models</a>
                </p>
                <p class="text-justify">
                    <a href="https://arxiv.org/abs/2206.01714">Compositional Visual Generation with Composable Diffusion Models</a>
                </p>
                <p class="text-justify">
                    <a href="https://arxiv.org/abs/2208.01626">Prompt-to-Prompt Image Editing with Cross Attention Control</a>
                </p>
            </div>
        </div>
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This project is funded by an unrestricted gift from Google.
                <br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
